# -*- coding: utf-8 -*-
"""Dissertation_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SImqcbauDgg4fwcK0UGixdyCPe15B8Ru
"""

DONT_RERUN_MAIN_STAGE2 = True
assert DONT_RERUN_MAIN_STAGE2

#0) Runtime & GPU check
#@title 🚀 Runtime & GPU check
import torch, platform
print("Python:", platform.python_version())
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
!nvidia-smi -L || true

#1) Mount Drive & define ALL paths (Final_output is the only sink)
#@title 📁 Paths — inputs (Project) → outputs (Final_output)
from google.colab import drive
drive.mount('/content/drive')

# === SOURCE (your cleaned data + scripts live here) ===
PROJ = "/content/drive/MyDrive/Project"      # matches your screenshots

RESUMES = f"{PROJ}/Resume_clean.csv"
JOBS    = f"{PROJ}/postings_clean.csv"
LABELS  = f"{PROJ}/labels_clean_top10_strict.csv"   # strict if present; change if needed

SKILLS_EN  = f"{PROJ}/skills_en.csv"
SKILLS_TXT = f"{PROJ}/Skills.txt"
TECH_TXT   = f"{PROJ}/Technology Skills.txt"

STAGE2_PY  = f"{PROJ}/graphmatch_stage2_graph.py"
STAGE3_PY  = f"{PROJ}/graphmatch_stage3_fairness.py"
STAGE4_PY  = f"{PROJ}/graphmatch_stage4_fairness_adjust.py"
AUDIT_TPR  = f"{PROJ}/fairlearn_audit_topk.py"

# existing stage outputs (we'll COPY from these)
ST2_SRC = f"{PROJ}/stage2_out"
ST3_SRC = f"{PROJ}/stage3_out"
ST4_SRC = f"{PROJ}/stage4_out"

# === DESTINATION (pilot deliverables) ===
OUT_ROOT = "/content/drive/MyDrive/Final_output"
ST2 = f"{OUT_ROOT}/stage2"
ST3 = f"{OUT_ROOT}/stage3"
ST4 = f"{OUT_ROOT}/stage4"

import os, shutil, glob
for p in (OUT_ROOT, ST2, ST3, ST4):
    os.makedirs(p, exist_ok=True)

print("Project:", PROJ)
print("Outputs:", OUT_ROOT)

#2) Install deps
#@title 🧰 Deps
!pip -q install pandas numpy scipy scikit-learn tqdm networkx fairlearn shap rank-bm25
import os, pandas as pd, numpy as np
import subprocess, os
print("OK")

# 🔧 Install PyG for Node2Vec GPU (run once before cmd_n2v)
import torch, subprocess, sys
ver = torch.__version__.split('+')[0]
cu  = torch.version.cuda.replace('.','')
wheel_idx = f"https://data.pyg.org/whl/torch-{ver}+cu{cu}.html"
subprocess.run([sys.executable,"-m","pip","install","-q",
                "pyg-lib","torch-scatter","torch-sparse","torch-cluster",
                "-f", wheel_idx], check=True)
subprocess.run([sys.executable,"-m","pip","install","-q","torch-geometric"], check=True)

#3) Copy Stage-2 artifacts (no re-run on main dataset)
#@title 📦 Copy Stage-2 outputs → Final_output/stage2 (no re-run)
import shutil, os, glob

assert os.path.exists(ST2_SRC), "Missing Project/stage2_out"
for src in glob.glob(f"{ST2_SRC}/*.csv"):
    shutil.copy2(src, ST2)
for src in glob.glob(f"{ST2_SRC}/*.json"):
    shutil.copy2(src, ST2)

print("Copied to", ST2)
!ls -lh "{ST2}" | sed -n '1,200p'

#4) Evaluate truth metrics (P@10 / nDCG@10 / MAP@10)
#@title 📈 Truth metrics → Final_output/stage2/overall_metrics_true.csv
import csv, math
from collections import defaultdict
import os, pandas as pd

assert os.path.exists(LABELS), "Labels file not found."

# load labels
L={}
with open(LABELS,"r",encoding="utf-8-sig",newline="") as f:
    r=csv.DictReader(f)
    jcol=next((c for c in r.fieldnames if c.lower() in ("job_id","job")), "job_id")
    rcol=next((c for c in r.fieldnames if c.lower() in ("resume_id","resume","id")), "resume_id")
    lcol=next((c for c in r.fieldnames if c.lower() in ("label","y","match_percent")), "label")
    for row in r:
        try:
            lab=int(float(row[lcol])); lab=1 if lab>0 else 0
        except:
            lab=0
        L[(str(row[jcol]),str(row[rcol]))]=lab

def eval_at10(path):
    d=defaultdict(list)
    with open(path,"r",encoding="utf-8-sig",newline="") as f:
        r=csv.DictReader(f)
        jcol=next((c for c in r.fieldnames if c.lower() in ("job_id","job")), "job_id")
        rcol=next((c for c in r.fieldnames if c.lower() in ("resume_id","resume","id")), "resume_id")
        rkcol=next((c for c in r.fieldnames if "rank" in c.lower()), None)
        scol=next((c for c in r.fieldnames if c.lower() in ("score","tfidf","bm25","sim","similarity")), None)
        for row in r:
            j=str(row[jcol]); rid=str(row[rcol])
            if rkcol and row.get(rkcol):
                try: rk=int(row[rkcol])
                except: rk=10**9
            elif scol and row.get(scol):
                try: rk=-float(row[scol])
                except: rk=10**9
            else:
                rk=10**9
            d[j].append((rk,rid))
    P=0; DCG=0; IDCG=0; AP=0; jobs=0
    for j,rows in d.items():
        rows=sorted(rows)[:10]
        rels=[L.get((j,ri),0) for _,ri in rows]
        jobs+=1
        P+=sum(rels)/10.0
        dcg=sum(1.0/math.log2(i+1) for i,r in enumerate(rels,1) if r==1)
        pos=sum(rels); idcg=sum(1.0/math.log2(i+1) for i in range(1,pos+1))
        DCG+=dcg; IDCG+= (idcg if idcg>0 else 1.0)
        seen=0; prec=[]
        for i,r in enumerate(rels,1):
            if r==1: seen+=1; prec.append(seen/i)
        AP += (sum(prec)/len(prec)) if prec else 0.0
    if jobs==0:
        return dict(p_at_10=None, **{"nDCG@10":None}, **{"MAP@10":None}, jobs_scored=0)
    return dict(p_at_10=round(P/jobs,4),
                **{"nDCG@10":round((DCG/IDCG),4)},
                **{"MAP@10":round(AP/jobs,4)},
                jobs_scored=jobs)

candidates = {
  "job_to_resumes_top10_tfidf.csv": "tfidf",
  "job_to_resumes_top10_bm25.csv":  "bm25",
  "job_to_resumes_top10_node2vec.csv": "node2vec",
  "job_to_resumes_top10_lightgcn.csv": "lightgcn",
  "job_to_resumes_top10_graphsage.csv": "graphsage",
  "job_to_resumes_top10.csv": "default"
}

rows=[]
for fname, tag in candidates.items():
    p=f"{ST2}/{fname}"
    if os.path.exists(p):
        rows.append(dict(source=tag, **eval_at10(p)))

df=pd.DataFrame(rows)
display(df)
df.to_csv(f"{ST2}/overall_metrics_true.csv", index=False, encoding="utf-8-sig")
df.to_csv(f"{ST2}/overall_metrics.csv", index=False, encoding="utf-8-sig")
print("Saved:", f"{ST2}/overall_metrics_true.csv")

#5) Stage-3 (pre-audit). If you already have one, we copy it; else we compute it

#@title ⚖️ Stage-3 pre-audit → Final_output/stage3/fairlearn_pre.csv
import os, shutil, subprocess, pandas as pd

dest_pre = f"{ST3}/fairlearn_pre.csv"
src_pre  = f"{ST3_SRC}/fairlearn_pre.csv"

if os.path.exists(src_pre):
    shutil.copy2(src_pre, dest_pre)
    print("Copied existing pre-audit:", dest_pre)
else:
    # compute using TF-IDF list as baseline
    base_list = f"{ST2}/job_to_resumes_top10_tfidf.csv"
    assert os.path.exists(base_list), "Need TF-IDF job_to_resumes_top10_tfidf.csv"
    cmd = [
      "python", STAGE3_PY,
      "--recommendations", base_list,
      "--resumes", RESUMES,
      "--protected_attrs", "Gender", "Ethnicity",
      "--top_k", "10",
      "--out_csv", dest_pre
    ]
    print("$", " ".join(cmd))
    subprocess.run(cmd, check=True)

display(pd.read_csv(dest_pre, encoding="utf-8-sig").head())

#6) Stage-4 re-ranking → adjusted top-K + fairness weights, then post-audit (TPR)

#@title ♻️ Stage-4 adjusted rankings + weights; post-audit with TPR
import subprocess, os, pandas as pd

adj_csv = f"{ST4}/job_to_resumes_top10_adjusted.csv"
wts_csv = f"{ST4}/fairness_weights.csv"
post_csv= f"{ST4}/fairlearn_post.csv"

base_list = f"{ST2}/job_to_resumes_top10_tfidf.csv"   # adjust TF-IDF; change if needed
pre_csv   = f"{ST3}/fairlearn_pre.csv"

# Stage-4: adjust
cmd4 = [
  "python", STAGE4_PY,
  "--recommendations", base_list,
  "--resumes", RESUMES,
  "--resume_id_col", "ID",
  "--job_id_col", "job_id",
  "--metrics", pre_csv,
  "--protected_attrs", "Gender", "Ethnicity",
  "--top_k", "10",
  "--output_adjusted", adj_csv,
  "--output_weights",  wts_csv
]
print("$", " ".join(cmd4))
subprocess.run(cmd4, check=True)

# Post-audit: selection + TPR (equal opportunity)
cmdpost = [
  "python", AUDIT_TPR,
  "--selected", adj_csv,
  "--resumes", RESUMES,
  "--sensitive_cols", "Gender", "Ethnicity",
  "--labels", LABELS,
  "--top_k", "10",
  "--out_csv", post_csv
]
print("$", " ".join(cmdpost))
subprocess.run(cmdpost, check=True)

print("Saved:")
print("  ", adj_csv)
print("  ", wts_csv)
print("  ", post_csv)

display(pd.read_csv(post_csv, encoding="utf-8-sig").head())

#7) Final snapshot (what the dashboard should point to)

#@title ✅ Pilot deliverables summary (for report/dashboard)
import json, os, pandas as pd

summary = {
  "stage2_dir": ST2,
  "metrics": [f"{ST2}/overall_metrics_true.csv", f"{ST2}/overall_metrics.csv"],
  "fairness_pre": f"{ST3}/fairlearn_pre.csv",
  "adjusted_topK": f"{ST4}/job_to_resumes_top10_adjusted.csv",
  "fairness_weights": f"{ST4}/fairness_weights.csv",
  "fairness_post": f"{ST4}/fairlearn_post.csv",
}
print(json.dumps(summary, indent=2))

#8) Kaggle API (variable slot for your key)
#@title 🔐 Kaggle API setup (for Perfect-Fit, optional)
KAGGLE_USERNAME = "arashmehrdad"  #@param {type:"string"}
KAGGLE_KEY      = "91b7c66433b1bf3202b4824542a17281"   #@param {type:"string"}
KAGGLE_DATASET  = "mukund23/a-perfect-fit"  #@param {type:"string"}

import os, json, pathlib, stat
!pip -q install kaggle >/dev/null
home = pathlib.Path.home(); kag_dir = home/".kaggle"
kag_dir.mkdir(exist_ok=True)
cfg = kag_dir/"kaggle.json"
cfg.write_text(json.dumps({"username": KAGGLE_USERNAME, "key": KAGGLE_KEY}))
cfg.chmod(stat.S_IRUSR | stat.S_IWUSR)
os.environ["KAGGLE_USERNAME"]=KAGGLE_USERNAME
os.environ["KAGGLE_KEY"]=KAGGLE_KEY

!kaggle --version

#@title 🧪 Perfect-Fit: rescue + build (one-shot) — with auto-install for pdfminer
import os, re, csv, glob, zipfile, shutil, logging, subprocess, sys
from pathlib import Path

# --- make sure pdfminer.six is available ---
try:
    from pdfminer.high_level import extract_text
except Exception:
    subprocess.run([sys.executable, "-m", "pip", "install", "-q", "pdfminer.six"], check=True)
    from pdfminer.high_level import extract_text

# Silence noisy pdfminer logs
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# === CONFIG ===
PF_ROOT  = f"{OUT_ROOT}/perfect_fit"    # OUT_ROOT must be defined earlier
PF_RAW   = f"{PF_ROOT}/raw"
PF_PROC  = f"{PF_ROOT}/processed"
KAGGLE_DATASET = globals().get("KAGGLE_DATASET", "mukund23/a-perfect-fit")

os.makedirs(PF_RAW, exist_ok=True)
os.makedirs(PF_PROC, exist_ok=True)

def is_empty(d: str) -> bool:
    return not any(Path(d).rglob("*"))

# --- 0) Rescue into PF_RAW ---
def rescue_into_pf_raw():
    moved = False
    # A) copy '/content/**/dataset' if present
    for src in sorted(glob.glob("/content/**/dataset", recursive=True), key=len, reverse=True):
        if is_empty(PF_RAW):
            dst = os.path.join(PF_RAW, "dataset")
            if not os.path.exists(dst):
                shutil.copytree(src, dst)
                print(f"✅ Copied dataset folder from {src} → {dst}")
                moved = True
                break
    # B) copy+extract any a-perfect-fit*.zip under /content
    if is_empty(PF_RAW):
        zips = glob.glob("/content/**/a-perfect-fit*.zip", recursive=True)
        if zips:
            zsrc = zips[0]
            zdst = os.path.join(PF_RAW, os.path.basename(zsrc))
            shutil.copy2(zsrc, zdst)
            with zipfile.ZipFile(zdst) as zz: zz.extractall(PF_RAW)
            os.remove(zdst)
            print(f"✅ Copied & extracted zip into {PF_RAW}")
            moved = True
    # C) extract any zips already in PF_RAW
    for z in glob.glob(PF_RAW + "/*.zip"):
        with zipfile.ZipFile(z) as zz: zz.extractall(PF_RAW)
        os.remove(z)
        print(f"✅ Extracted local zip {os.path.basename(z)}")
        moved = True
    return moved

_ = rescue_into_pf_raw()

# --- 1) If still empty, download to PF_RAW via Kaggle ---
if is_empty(PF_RAW):
    print("ℹ️ PF_RAW empty — downloading Perfect-Fit to the correct folder…")
    subprocess.run([sys.executable,"-m","pip","install","-q","kaggle"], check=True)
    os.environ.setdefault("KAGGLE_CONFIG_DIR", str(Path.home()/".kaggle"))
    !kaggle datasets download -d "$KAGGLE_DATASET" -p "$PF_RAW" -w
    for z in glob.glob(PF_RAW + "/*.zip"):
        with zipfile.ZipFile(z) as zz: zz.extractall(PF_RAW)
        os.remove(z)

# --- 2) Find mapping CSV (tolerant, recursive) ---
root = Path(PF_RAW)
def norm(s: str) -> str: return re.sub(r"[^a-z0-9]+", "", s.lower())

def find_mapping_csv_tol(root: Path) -> Path | None:
    best = None
    for p in root.rglob("*.csv"):
        try:
            with open(p, "r", encoding="utf-8-sig", newline="") as f:
                r = csv.DictReader(f)
                hdr = [norm(h) for h in (r.fieldnames or [])]
        except Exception:
            continue
        has_id = any(x in hdr for x in ["candidateid","candidate_id","candidate id","resumeid","resume_id","resume id","id","filename","file","resume"])
        has_y  = any(x in hdr for x in ["match","matchpercentage","match percentage","score","target","label","percentage"])
        if not (has_id and has_y): continue
        score = (10 if p.name.lower()=="train.csv" else 0) + (3 if "train" in str(p).lower() else 0) + (1 if "dataset" in str(p).lower() else 0)
        if best is None or score > best[0]: best = (score, p)
    return best[1] if best else None

def read_match_pairs(path: Path):
    with open(path, "r", encoding="utf-8-sig", newline="") as f:
        r = csv.DictReader(f)
        hdr_raw = [h.strip() for h in (r.fieldnames or [])]
        low = [h.lower() for h in hdr_raw]
        id_cands = ["candidateid","candidate_id","candidate id","resumeid","resume_id","resume id","id","filename","file","resume"]
        y_cands  = ["match","match percentage","match_percentage","matchpercent","score","target","label","percentage"]
        def pick(cands, lowhdr):
            for n in cands:
                if n in lowhdr: return lowhdr.index(n)
            for i,h in enumerate(lowhdr):
                if any(n in h for n in cands): return i
            return None
        id_idx = pick(id_cands, low); y_idx = pick(y_cands, low)
        if id_idx is None or y_idx is None: raise ValueError(f"Could not identify ID/label columns in {path.name}. Headers: {hdr_raw}")
        rows=[]
        for row in r:
            vals = list(row.values())
            cid  = str(vals[id_idx]).strip().removesuffix(".pdf")
            raw  = str(vals[y_idx])
            try: score = float(re.sub(r"[^\d.-]+","", raw))
            except: score = 0.0
            rows.append((cid, score))
        return rows

def extract_pdf_text(pdf_path: Path) -> str:
    try:
        t = extract_text(str(pdf_path)) or ""
        return re.sub(r"\s+"," ", t).strip()
    except Exception:
        return ""

mapping_csv = find_mapping_csv_tol(root)
assert mapping_csv is not None, "Could not find a CSV with CandidateID + Match/Score. Ensure download succeeded."
pairs = read_match_pairs(mapping_csv)
print("Mapping CSV:", mapping_csv.relative_to(root), "rows:", len(pairs))

cand_ids = {cid.lower() for cid,_ in pairs}
pdfs = list(root.rglob("*.pdf"))
assert pdfs, "No PDFs found under perfect_fit/raw."

job_pdf = next((p for p in pdfs if ("job" in p.name.lower() and "description" in p.name.lower())), None)
if job_pdf is None: job_pdf = min(pdfs, key=lambda x: x.stat().st_size)

resume_pdfs = [p for p in pdfs if p.stem.lower() in cand_ids]
print("Job PDF:", job_pdf.name)
print("Matched resume PDFs:", len(resume_pdfs))

# --- write outputs ---
with open(f"{PF_PROC}/postings_clean.csv","w",encoding="utf-8-sig",newline="") as f:
    w = csv.DictWriter(f, fieldnames=["job_id","title","description"]); w.writeheader()
    w.writerow({"job_id":"PF1","title":"A Perfect Fit – Single JD","description": extract_pdf_text(job_pdf)})

with open(f"{PF_PROC}/Resume_clean.csv","w",encoding="utf-8-sig",newline="") as f:
    w = csv.DictWriter(f, fieldnames=["ID","Resume_str","Gender","Ethnicity"]); w.writeheader()
    for p in sorted(resume_pdfs):
        w.writerow({"ID": p.stem, "Resume_str": extract_pdf_text(p), "Gender":"unknown", "Ethnicity":"unknown"})

top10 = {cid for cid,_ in sorted(pairs, key=lambda x:x[1], reverse=True)[:10]}
with open(f"{PF_PROC}/labels_perfectfit_top10.csv","w",encoding="utf-8-sig",newline="") as f:
    w = csv.DictWriter(f, fieldnames=["job_id","resume_id","label","match_percent"]); w.writeheader()
    for cid, score in pairs:
        w.writerow({"job_id":"PF1","resume_id":cid,"label": 1 if cid in top10 else 0,"match_percent": score})

print("Wrote:")
print("  ", f"{PF_PROC}/postings_clean.csv")
print("  ", f"{PF_PROC}/Resume_clean.csv")
print("  ", f"{PF_PROC}/labels_perfectfit_top10.csv")

# Quick PF schema + consistency check
import pandas as pd, os

PF_PROC = "/content/drive/MyDrive/Final_output/perfect_fit/processed"
post = pd.read_csv(f"{PF_PROC}/postings_clean.csv", encoding="utf-8-sig")
res  = pd.read_csv(f"{PF_PROC}/Resume_clean.csv", encoding="utf-8-sig")
labs = pd.read_csv(f"{PF_PROC}/labels_perfectfit_top10.csv", encoding="utf-8-sig")

print("postings_clean:", post.shape, post.columns.tolist()); display(post.head(1))
print("Resume_clean:", res.shape, res.columns.tolist()); display(res.head(3))
print("labels:", labs.shape, labs['label'].value_counts(dropna=False))

# consistency: every labelled resume exists
missing = set(labs['resume_id'].astype(str)) - set(res['ID'].astype(str))
print("Missing resumes referenced in labels:", len(missing))
assert post.loc[0, 'job_id'] == 'PF1'
assert labs['job_id'].nunique() == 1 and labs['job_id'].iloc[0] == 'PF1'
assert res['ID'].nunique() >= 10, "Need at least 10 resumes"
print("✅ PF files look consistent.")

#TF-IDF (GPU) + BM25 (CPU)

PF_ST2 = f"{PF_ROOT}/stage2"; os.makedirs(PF_ST2, exist_ok=True)

# TF-IDF on GPU
cmd_tfidf = [
  "python", STAGE2_PY,
  "--resumes", f"{PF_PROC}/Resume_clean.csv",
  "--jobs",    f"{PF_PROC}/postings_clean.csv",
  "--skills_en", SKILLS_EN, "--skills_txt", SKILLS_TXT, "--tech_txt", TECH_TXT,
  "--top_k","10","--direction","both",
  "--baseline","tfidf",
  "--use_gpu_tfidf",                 # <<< enable CUDA
  "--tfidf_gpu_dtype","float16",     # <<< halve memory, usually fine for ranking
  "--tfidf_gpu_chunk","6000",        # <<< tune if you hit OOM; lower = safer
  "--output_dir", PF_ST2
]
subprocess.run(cmd_tfidf, check=True)

# BM25 on CPU (unchanged)
cmd_bm25 = [
  "python", STAGE2_PY,
  "--resumes", f"{PF_PROC}/Resume_clean.csv",
  "--jobs",    f"{PF_PROC}/postings_clean.csv",
  "--skills_en", SKILLS_EN, "--skills_txt", SKILLS_TXT, "--tech_txt", TECH_TXT,
  "--top_k","10","--direction","both",
  "--baseline","bm25",
  "--bm25_workers","2","--bm25_chunk","500",
  "--output_dir", PF_ST2
]
subprocess.run(cmd_bm25, check=True)

# PyTorch Geometric install matched to your Torch/CUDA
import torch, sys, subprocess
ver = torch.__version__.split('+')[0]
cu  = torch.version.cuda.replace('.','')
wheel = f"https://data.pyg.org/whl/torch-{ver}+cu{cu}.html"
subprocess.run([sys.executable,"-m","pip","install","-q",
                "pyg-lib","torch-scatter","torch-sparse","torch-cluster",
                "-f", wheel], check=True)
subprocess.run([sys.executable,"-m","pip","install","-q","torch-geometric"], check=True)

# sanity
import torch_geometric
print("Torch:", torch.__version__, "CUDA:", torch.version.cuda)
print("PyG OK")

# 🩹 PF: provide an empty --related file (so Path(args.related) isn't None)
PF_PROC   = "/content/drive/MyDrive/Final_output/perfect_fit/processed"
PF_ST2    = "/content/drive/MyDrive/Final_output/perfect_fit/stage2"
STAGE2_PY = "/content/drive/MyDrive/Project/graphmatch_stage2_graph.py"

# Create stub if missing
import os, pandas as pd
RELATED = f"{PF_PROC}/_empty_related.csv"
if not os.path.exists(RELATED):
    pd.DataFrame(columns=["skill","related_skill","weight"]).to_csv(
        RELATED, index=False, encoding="utf-8-sig"
    )
print("Using RELATED =", RELATED)

# Run Node2Vec (GPU) with explicit columns + the stub
!python -u "$STAGE2_PY" \
  --resumes "$PF_PROC/Resume_clean.csv" \
  --jobs    "$PF_PROC/postings_clean.csv" \
  --resume_id_col ID --resume_text_col Resume_str \
  --job_id_col job_id --job_text_col description \
  --skills_en "/content/drive/MyDrive/Project/skills_en.csv" \
  --skills_txt "/content/drive/MyDrive/Project/Skills.txt" \
  --tech_txt   "/content/drive/MyDrive/Project/Technology Skills.txt" \
  --related    "$RELATED" \
  --top_k 10 --direction both \
  --use_gpu_node2vec \
  --n2v_dim 128 --n2v_epochs 20 \
  --n2v_walk_length 40 --n2v_context 10 \
  --n2v_walks_per_node 15 \
  --n2v_loader_bs 2048 --n2v_loader_workers 2 \
  --output_dir "$PF_ST2"

#@title ⚡ GPU LightGCN & GraphSAGE on Perfect-Fit + P@10 (fixed negatives + progress)
import os, csv, math, random, copy, time, torch, pandas as pd
import torch.nn as nn
from collections import defaultdict
from tqdm.auto import tqdm

# --------- CONFIG ----------
PF_PROC = "/content/drive/MyDrive/Final_output/perfect_fit/processed"
PF_ST2  = "/content/drive/MyDrive/Final_output/perfect_fit/stage2"
labs    = f"{PF_PROC}/labels_perfectfit_top10.csv"
HIDDEN = 32; LAYERS = 2
MAX_EPOCHS=15; PATIENCE=3; EVAL_EVERY=2
BATCH_BPR=2048; REG_L2=1e-4; SEED=42
random.seed(SEED); torch.manual_seed(SEED)

def stage(msg): print(f"\n=== {msg} ===", flush=True)

def read_rows(p):
    with open(p,"r",encoding="utf-8-sig",newline="") as f:
        for r in csv.DictReader(f): yield r

def load_labels(path):
    return {(r["job_id"],r["resume_id"]): int(float(r["label"]))
            for r in csv.DictReader(open(path,encoding="utf-8-sig"))}

def eval_p10(job_top, L):
    num=den=0
    for j,lst in job_top.items():
        rels=[L.get((j,ri),0) for ri,_ in lst[:10]]
        num+=sum(rels); den+=10
    return num/den if den else 0.0

# --------- STAGE 1: Load edges (TF-IDF ∪ BM25) and FULL resume set ----------
stage("STAGE 1/5  Load edges + full resume universe")
tf = os.path.join(PF_ST2, "job_to_resumes_top10_tfidf.csv")
bm = os.path.join(PF_ST2, "job_to_resumes_top10_bm25.csv")
assert os.path.exists(tf) and os.path.exists(bm), "Run PF TF-IDF+BM25 first."

# Edges from TF-IDF + BM25
jobs=set(); res_in_edges=set(); edges=set()
for p in (tf,bm):
    for r in read_rows(p):
        j=r["job_id"].strip(); ri=r["resume_id"].strip()
        jobs.add(j); res_in_edges.add(ri); edges.add((j,ri))

# FULL resume universe from Resume_clean.csv (≈90)
res_all=set(pd.read_csv(f"{PF_PROC}/Resume_clean.csv", encoding="utf-8-sig")["ID"].astype(str))
print(f"[PF check] jobs={len(jobs)} (expect 1), resumes_in_edges={len(res_in_edges)}, resumes_total={len(res_all)}, edges={len(edges)}")
assert jobs == {"PF1"}, "This doesn't look like Perfect-Fit (job_id should be PF1)."

# --------- STAGE 2: Build graph tensors (nodes = 1 job + ALL resumes) ----------
stage("STAGE 2/5  Build bipartite graph tensors")
job2i={j:i for i,j in enumerate(sorted(jobs))}
resumes_sorted = sorted(res_all)
res2i={r:i for i,r in enumerate(resumes_sorted)}
nJ=len(job2i); nR=len(res2i); n=nJ+nR

def ji(j): return job2i[j]
def ri(r): return nJ+res2i[r]

# Edges only where we have interactions, but include both directions
edge_index=[(ji(j), ri(r)) for (j,r) in edges if r in res2i]
edge_index += [(v,u) for (u,v) in edge_index]

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device, (torch.cuda.get_device_name(0) if torch.cuda.is_available() else ""))
idx=torch.tensor(edge_index, dtype=torch.long, device=device).t().contiguous()
print(f"Tensors: nJ={nJ}, nR={nR}, nodes={n}, undirected edges={idx.size(1)}")

def normalized_adj(n_nodes, idx):
    val=torch.ones(idx.size(1), device=device)
    deg=torch.zeros(n_nodes, device=device).scatter_add_(0, idx[0], torch.ones_like(idx[0], dtype=torch.float32))
    deg=deg.clamp(min=1.0); d=deg.pow(-0.5); v=d[idx[0]]*val*d[idx[1]]
    return torch.sparse_coo_tensor(idx, v, (n_nodes,n_nodes), device=device).coalesce()

t0=time.perf_counter(); A=normalized_adj(n, idx)
torch.cuda.synchronize() if device.type=="cuda" else None
print(f"Adjacency built in {time.perf_counter()-t0:.3f}s")

# Precompute negative pool for the (single) job: resumes NOT connected
connected_res_idx = {ri(r) for (_,r) in edges if r in res2i}
neg_pool = [ri(r) for r in resumes_sorted if ri(r) not in connected_res_idx]
if not neg_pool:
    # Fallback: allow any resume as "negative" (still avoids infinite loop)
    neg_pool = [ri(r) for r in resumes_sorted]
neg_pool = torch.tensor(neg_pool, dtype=torch.long, device=device)

pairs = [(ji(j), ri(r)) for (j,r) in edges if r in res2i]  # positives (job->resume)
pairs = random.sample(pairs, k=len(pairs))  # shuffle once

def sample_bpr(batch=BATCH_BPR):
    k=min(batch, len(pairs))
    pos=random.sample(pairs, k=k)
    pj=torch.tensor([u for (u,v) in pos], device=device)
    pr=torch.tensor([v for (u,v) in pos], device=device)
    # negatives from pool (vectorized, guaranteed finite)
    nr = neg_pool[torch.randint(0, neg_pool.numel(), (k,), device=device)]
    # (optional) avoid pr==nr
    same = nr.eq(pr)
    if same.any():
        nr[same] = neg_pool[torch.randint(0, neg_pool.numel(), (int(same.sum().item()),), device=device)]
    return pj,pr,nr

# --------- Labels ----------
L = load_labels(labs); print("Labels loaded:", len(L))

def split(X):
    J=torch.nn.functional.normalize(X[:nJ],p=2,dim=1)
    R=torch.nn.functional.normalize(X[nJ:],p=2,dim=1)
    return J,R

def topk_job(J,R,K=10):
    inv_r={v:k for k,v in res2i.items()}; inv_j={v:k for k,v in job2i.items()}
    S=J@R.t()
    tv,ti=torch.topk(S,k=min(K,R.size(0)),dim=1)
    d={}
    for jidx in range(J.size(0)):
        j=inv_j[jidx]
        d[j]=[(inv_r[ti[jidx,k].item()], float(tv[jidx,k].item())) for k in range(tv.size(1))]
    return d

def bpr(u,pos,neg,reg=REG_L2):
    loss = -torch.log(torch.sigmoid((u*pos).sum(1)-(u*neg).sum(1))).mean()
    return loss + reg*(u.norm(2)**2+pos.norm(2)**2+neg.norm(2)**2)/u.size(0)

# --------- STAGE 3: Train LightGCN (progress + ETA) ----------
stage("STAGE 3/5  Train LightGCN (progress & ETA)")

class LightGCN(nn.Module):
    def __init__(self, n, d=HIDDEN, L=LAYERS):
        super().__init__()
        self.emb=nn.Embedding(n,d); nn.init.xavier_uniform_(self.emb.weight)
        self.L=L
    def forward(self, A):
        x_list=[self.emb.weight]; x=self.emb.weight
        for _ in range(self.L):
            x=torch.sparse.mm(A,x); x_list.append(x)
        return torch.stack(x_list,0).mean(0)

lgcn=LightGCN(n).to(device)
opt=torch.optim.Adam(lgcn.parameters(), lr=2e-3)
best=-1; best_state=None; stall=0; t_start=time.perf_counter()

for ep in tqdm(range(1, MAX_EPOCHS+1), leave=False, desc="LightGCN"):
    pj,pr,nr = sample_bpr()
    X=lgcn(A); loss=bpr(X[pj],X[pr],X[nr])
    opt.zero_grad(); loss.backward(); opt.step()
    if ep % EVAL_EVERY == 0 or ep==1:
        with torch.no_grad():
            J,R=split(lgcn(A)); p10=eval_p10(topk_job(J,R), L)
        elapsed=time.perf_counter()-t_start
        done=ep; remain=max((MAX_EPOCHS-done)/max(done,1)*elapsed, 0.0)
        tqdm.write(f"[LightGCN] ep{ep:02d} loss={loss.item():.4f} P@10={p10:.4f} ETA={remain:.1f}s")
        if p10>best+1e-6: best=p10; best_state=copy.deepcopy(lgcn.state_dict()); stall=0
        else: stall+=1
        if stall>=PATIENCE:
            tqdm.write("LightGCN early stop."); break
if best_state is not None: lgcn.load_state_dict(best_state)
with torch.no_grad(): J,R=split(lgcn(A)); job_top_gcn=topk_job(J,R)
p10_gcn=eval_p10(job_top_gcn, L)
print(f"✅ LightGCN done — P@10={p10_gcn:.4f}")

# --------- STAGE 4: Train GraphSAGE (progress + ETA) ----------
stage("STAGE 4/5  Train GraphSAGE (progress & ETA)")

class MeanSAGE(nn.Module):
    def __init__(self,n,h=HIDDEN):
        super().__init__()
        self.x=nn.Embedding(n,h); nn.init.xavier_uniform_(self.x.weight)
        self.idx=idx; self.n=n
        self.deg=torch.bincount(self.idx[0], minlength=n).clamp(min=1).to(device)
        self.lin1=nn.Linear(h*2,h); self.lin2=nn.Linear(h*2,h)
    def mean(self,X):
        row,col=self.idx[0], self.idx[1]
        m=X[col]; out=torch.zeros_like(X); out.index_add_(0,row,m)
        return out/self.deg.unsqueeze(1)
    def forward(self):
        x0=self.x.weight
        h1=self.lin1(torch.cat([x0,self.mean(x0)],1)).relu()
        h2=self.lin2(torch.cat([h1,self.mean(h1)],1))
        return h2

sage=MeanSAGE(n).to(device)
opt=torch.optim.Adam(sage.parameters(), lr=3e-3, weight_decay=1e-5)
best=-1; best_state=None; stall=0; t_start=time.perf_counter()

for ep in tqdm(range(1, MAX_EPOCHS+1), leave=False, desc="GraphSAGE"):
    pj,pr,nr = sample_bpr()
    X=sage(); loss=bpr(X[pj],X[pr],X[nr])
    opt.zero_grad(); loss.backward(); opt.step()
    if ep % EVAL_EVERY == 0 or ep==1:
        with torch.no_grad():
            J,R=split(sage()); p10=eval_p10(topk_job(J,R), L)
        elapsed=time.perf_counter()-t_start
        done=ep; remain=max((MAX_EPOCHS-done)/max(done,1)*elapsed, 0.0)
        tqdm.write(f"[GraphSAGE] ep{ep:02d} loss={loss.item():.4f} P@10={p10:.4f} ETA={remain:.1f}s")
        if p10>best+1e-6: best=p10; best_state=copy.deepcopy(sage.state_dict()); stall=0
        else: stall+=1
        if stall>=PATIENCE:
            tqdm.write("GraphSAGE early stop."); break
if best_state is not None: sage.load_state_dict(best_state)
with torch.no_grad(): J,R=split(sage()); job_top_sage=topk_job(J,R)
p10_sage=eval_p10(job_top_sage, L)
print(f"✅ GraphSAGE done — P@10={p10_sage:.4f}")

# --------- STAGE 5: Save outputs ----------
stage("STAGE 5/5  Save top-10 files")
def save_job2res(path, d, src):
    with open(path,"w",encoding="utf-8-sig",newline="") as f:
        w=csv.DictWriter(f, fieldnames=["job_id","resume_id","score","rank","source"]); w.writeheader()
        for j,lst in d.items():
            for rk,(rid,sc) in enumerate(lst,1):
                w.writerow({"job_id": j, "resume_id": rid, "score": f"{sc:.6f}", "rank": rk, "source": src})

out_gcn  = os.path.join(PF_ST2,"job_to_resumes_top10_lightgcn.csv")
out_sage = os.path.join(PF_ST2,"job_to_resumes_top10_graphsage.csv")
save_job2res(out_gcn,  job_top_gcn,  "lightgcn")
save_job2res(out_sage, job_top_sage, "graphsage")
print("Wrote:\n  ", out_gcn, "\n  ", out_sage)
print("🎯 Summary — LightGCN P@10:", round(p10_gcn,4), "| GraphSAGE P@10:", round(p10_sage,4))

#12-bis) (Optional) Node2Vec on your main dataset → Final_output/stage2/job_to_resumes_top10_node2vec.csv
#@title 🧭 (Optional) Node2Vec on MAIN dataset from existing TF-IDF+BM25 lists
DO_NODE2VEC_MAIN = False  # ← set True to run
if DO_NODE2VEC_MAIN:
    !pip -q install node2vec gensim networkx >/dev/null
    import os, csv, math, numpy as np, networkx as nx
    from node2vec import Node2Vec

    tf = f"{ST2}/job_to_resumes_top10_tfidf.csv"
    bm = f"{ST2}/job_to_resumes_top10_bm25.csv"
    assert os.path.exists(tf) and os.path.exists(bm), "Need TF-IDF & BM25 outputs in Final_output/stage2"

    def read_rows(p):
        with open(p,"r",encoding="utf-8-sig",newline="") as f:
            for r in csv.DictReader(f):
                yield {k or "":("" if v is None else str(v)) for k,v in r.items()}
    edges=set(); jobs=set(); res=set()
    for p in (tf,bm):
        for r in read_rows(p):
            j=r.get("job_id","").strip(); ri=r.get("resume_id","").strip()
            if j and ri:
                edges.add((j,ri)); jobs.add(j); res.add(ri)

    G = nx.Graph()
    for j in jobs: G.add_node(f"j:{j}", bipartite=0)
    for r in res:  G.add_node(f"r:{r}", bipartite=1)
    for j,ri in edges:
        G.add_edge(f"j:{j}", f"r:{ri}", weight=1.0)

    print(f"[MAIN] Graph: |V|={G.number_of_nodes()}  |E|={G.number_of_edges()}  (this can be large)")

    n2v = Node2Vec(G, dimensions=64, walk_length=25, num_walks=200, p=1.0, q=1.0, workers=2, seed=42, quiet=True)
    w2v = n2v.fit(window=10, min_count=1, batch_words=10000, epochs=3)

    import numpy as np
    def vec(name:str): return w2v.wv[name]

    jobs_sorted = sorted(jobs); res_sorted = sorted(res)
    J = np.vstack([vec(f"j:{j}") for j in jobs_sorted])
    R = np.vstack([vec(f"r:{r}") for r in res_sorted])

    def l2norm(X): return X/(np.linalg.norm(X,axis=1,keepdims=True)+1e-12)
    Jn=l2norm(J); Rn=l2norm(R)

    # blockwise to reduce RAM
    K=10; m=len(jobs_sorted); n=len(res_sorted)
    from math import ceil
    B = max(1, (1024*1024*512)//(4*max(1,n)))  # ~512MB blocks
    rows=[]
    for s in range(0,m,B):
        e=min(m, s+B)
        Sc = Jn[s:e] @ Rn.T
        idx = np.argpartition(-Sc, K-1, axis=1)[:, :K]
        part = np.take_along_axis(Sc, idx, axis=1)
        ord  = np.argsort(-part, axis=1)
        for i,ji in enumerate(range(s,e)):
            j = jobs_sorted[ji]
            order = idx[i, ord[i]]
            vals  = part[i, ord[i]]
            for rk,ri_idx in enumerate(order, start=1):
                rows.append({"job_id": j, "resume_id": res_sorted[ri_idx], "score": f"{vals[rk-1]:.6f}", "rank": rk, "source":"node2vec"})
        del Sc, idx, part, ord
    MAIN_NODE2VEC = f"{ST2}/job_to_resumes_top10_node2vec.csv"
    with open(MAIN_NODE2VEC,"w",encoding="utf-8-sig",newline="") as f:
        w=csv.DictWriter(f, fieldnames=["job_id","resume_id","score","rank","source"]); w.writeheader()
        for r in rows: w.writerow(r)
    print("Saved:", MAIN_NODE2VEC)

